{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Dataset and DataLoader in PyTorch**\n",
    "\n",
    "PyTorch provides two key classes for handling data efficiently in machine learning workflows: the `Dataset` class and the `DataLoader` class. These two classes work together to streamline the process of loading, accessing, and feeding data to a model during training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Dataset Class**\n",
    "\n",
    "#### **Purpose**:\n",
    "The `Dataset` class is designed to represent your data as a collection of samples. Its main role is to provide a structured way to organize and access your dataset, no matter how or where the data is stored (e.g., in memory, on disk, or downloaded from the web). \n",
    "\n",
    "#### **Key Responsibilities**:\n",
    "1. **Abstraction**: \n",
    "   - Abstracts away how data is stored and provides a consistent interface to access individual data samples.\n",
    "2. **On-Demand Access**:\n",
    "   - Data samples are accessed on demand rather than being preloaded into memory, which is useful for large datasets.\n",
    "3. **Custom Data Loading**:\n",
    "   - Enables defining how data samples are loaded, transformed, or preprocessed (e.g., reading from files, normalizing images).\n",
    "\n",
    "#### **How It Works**:\n",
    "- The `Dataset` class is an **abstract base class**, meaning you typically create your own custom dataset by subclassing it.\n",
    "- It requires you to implement two key methods:\n",
    "  1. **`__len__`**: Returns the total number of samples in the dataset.\n",
    "  2. **`__getitem__`**: Fetches a single sample (data and label) given an index.\n",
    "\n",
    "---\n",
    "### **Why is the `__init__` method important in the `Dataset` class?**\n",
    "\n",
    "The `__init__` method is the **starting point** for initializing any custom dataset. It’s where you define how the data is stored, any preprocessing steps, and metadata. Without a well-defined `__init__` method, the `Dataset` cannot function properly because it wouldn’t know where or how to fetch the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose of `__init__` in the Dataset Class**\n",
    "\n",
    "1. **Data Initialization**:\n",
    "   - It loads or prepares the dataset that will later be accessed sample by sample. \n",
    "   - For example, it might load paths to images, read CSV files, or even prepare in-memory data structures.\n",
    "\n",
    "2. **Preprocessing Setup**:\n",
    "   - Any preprocessing (like resizing, normalization, or data augmentation) that applies to all samples can be set up here.\n",
    "   - For example, setting up a `transform` parameter to apply transformations consistently across all samples.\n",
    "\n",
    "3. **Metadata Definition**:\n",
    "   - Information like dataset labels, file paths, or dataset-specific properties (e.g., image dimensions) is initialized here.\n",
    "\n",
    "4. **Storing Variables**:\n",
    "   - Any variables that need to be shared across `__getitem__` calls are stored as attributes in `__init__`.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "\n",
    "When you subclass `Dataset`, the `__init__` method is executed **once** when you create the dataset object. This step is crucial for preparing the data so that later methods (`__len__` and `__getitem__`) can access it efficiently.\n",
    "\n",
    "### **Why Use the Dataset Class?**\n",
    "\n",
    "1. **Standardization**:\n",
    "   - Provides a consistent way to handle datasets regardless of their size or storage format.\n",
    "   \n",
    "2. **Memory Efficiency**:\n",
    "   - Large datasets can be loaded and accessed one sample at a time, preventing memory overflow.\n",
    "\n",
    "3. **Customizability**:\n",
    "   - You can implement any specific data-loading logic, such as loading images from directories, parsing CSV files, or handling multiple input types.\n",
    "\n",
    "4. **Seamless Integration**:\n",
    "   - It integrates well with PyTorch’s `DataLoader`, making it easier to handle batching, shuffling, and parallel loading.\n",
    "\n",
    "---\n",
    "\n",
    "### **DataLoader Class**\n",
    "\n",
    "#### **Purpose**:\n",
    "The `DataLoader` class is designed to handle **batching, shuffling, and parallel data loading**. It wraps around a `Dataset` and simplifies the process of feeding data to a model during training or evaluation.\n",
    "\n",
    "#### **Key Responsibilities**:\n",
    "1. **Batching**:\n",
    "   - Splits the data into manageable batches of a fixed size for efficient training.\n",
    "   \n",
    "2. **Shuffling**:\n",
    "   - Randomizes the order of samples to prevent learning patterns based on data order.\n",
    "\n",
    "3. **Parallelism**:\n",
    "   - Enables loading data using multiple worker threads or processes, speeding up data preparation for large datasets.\n",
    "\n",
    "4. **Collation**:\n",
    "   - Combines multiple samples (fetched by the `Dataset`) into a single batch, ensuring they are in the right format for model input.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**:\n",
    "The `DataLoader` takes a `Dataset` object as input and handles the following:\n",
    "1. Calls `__len__` from the `Dataset` to know the total number of samples.\n",
    "2. Calls `__getitem__` from the `Dataset` to fetch individual samples.\n",
    "3. Groups samples into batches according to the specified batch size.\n",
    "4. (Optional) Shuffles the data after every epoch if `shuffle=True`.\n",
    "5. Utilizes multiple worker processes (if specified) to load data in parallel.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use the DataLoader Class?**\n",
    "\n",
    "1. **Ease of Use**:\n",
    "   - Automatically handles batching and shuffling, removing the need for manual implementation.\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - Parallel loading improves data preparation speed, especially for large datasets.\n",
    "\n",
    "3. **Scalability**:\n",
    "   - Handles datasets of all sizes and works well for distributed or multi-GPU setups.\n",
    "\n",
    "4. **Flexibility**:\n",
    "   - Supports custom collation functions, allowing for specialized handling of complex datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Dataset and DataLoader Work Together**\n",
    "\n",
    "1. The `Dataset` defines how to access individual samples (e.g., data and labels) from your dataset.\n",
    "2. The `DataLoader` takes care of loading these samples in batches, shuffling them if needed, and using multiple workers to optimize speed.\n",
    "3. Together, they ensure your data pipeline is efficient, modular, and scalable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Key Differences**\n",
    "\n",
    "| **Aspect**       | **Dataset**                                      | **DataLoader**                                  |\n",
    "|-------------------|--------------------------------------------------|------------------------------------------------|\n",
    "| **Purpose**       | Provides access to individual data samples.      | Handles batching, shuffling, and parallelism.  |\n",
    "| **Focus**         | Defines how data is loaded and preprocessed.     | Focuses on optimizing data feeding to the model. |\n",
    "| **Customization** | You subclass it to define your custom logic.     | Offers arguments to control batching, shuffling, etc. |\n",
    "| **Usage**         | Fetches one sample at a time.                   | Combines samples into batches and processes them efficiently. |\n",
    "\n",
    "---\n"
   ],
   "id": "6077da890249772d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-16T11:16:31.363151Z",
     "start_time": "2025-01-16T11:16:30.430886Z"
    }
   },
   "source": "import torch",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:12:59.170208Z",
     "start_time": "2025-01-17T09:12:54.642186Z"
    }
   },
   "cell_type": "code",
   "source": "from sklearn.datasets import make_classification\n",
   "id": "78460fcc1fc3991f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:14:16.012806Z",
     "start_time": "2025-01-17T09:14:15.990912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# step 1 create synthetic classification data using sklearn \n",
    "X,y  = make_classification(\n",
    "    n_samples=10,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    random_state=42,\n",
    ")"
   ],
   "id": "26574ebfa8d33fc7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:14:28.494291Z",
     "start_time": "2025-01-17T09:14:28.480240Z"
    }
   },
   "cell_type": "code",
   "source": "X",
   "id": "4ecafade5fcc7d00",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.06833894, -0.97007347],\n",
       "       [-1.14021544, -0.83879234],\n",
       "       [-2.8953973 ,  1.97686236],\n",
       "       [-0.72063436, -0.96059253],\n",
       "       [-1.96287438, -0.99225135],\n",
       "       [-0.9382051 , -0.54304815],\n",
       "       [ 1.72725924, -1.18582677],\n",
       "       [ 1.77736657,  1.51157598],\n",
       "       [ 1.89969252,  0.83444483],\n",
       "       [-0.58723065, -1.97171753]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:14:33.796129Z",
     "start_time": "2025-01-17T09:14:33.781892Z"
    }
   },
   "cell_type": "code",
   "source": "y",
   "id": "520c5e6330aa1ed0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:16:13.532075Z",
     "start_time": "2025-01-17T09:16:13.497993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert the data to pytorch tensor\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)"
   ],
   "id": "7601caabb537e385",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T14:26:31.565480Z",
     "start_time": "2025-01-17T14:26:31.534161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n"
   ],
   "id": "2ad91f55570998e0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T14:26:38.396411Z",
     "start_time": "2025-01-17T14:26:38.381683Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = customDataset(X, y)",
   "id": "e332e36fa1e5a51f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T14:26:43.991632Z",
     "start_time": "2025-01-17T14:26:43.977290Z"
    }
   },
   "cell_type": "code",
   "source": "len(dataset)",
   "id": "840c79dbcc470fe3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T14:27:04.949679Z",
     "start_time": "2025-01-17T14:27:04.928605Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[0]",
   "id": "4bc79fcb627ad94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.0683, -0.9701]), tensor(1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T14:28:25.407010Z",
     "start_time": "2025-01-17T14:28:25.367880Z"
    }
   },
   "cell_type": "code",
   "source": "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)",
   "id": "c1ecd49e0e8a6300",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T14:29:30.001299Z",
     "start_time": "2025-01-17T14:29:29.933963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch_features , batch_labels in dataloader:\n",
    "    print(batch_features)\n",
    "    print(batch_labels)\n",
    "    print('-'*50)"
   ],
   "id": "c2d3ddb18448ec67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7273, -1.1858],\n",
      "        [-0.5872, -1.9717]])\n",
      "tensor([1, 0])\n",
      "--------------------------------------------------\n",
      "tensor([[ 1.0683, -0.9701],\n",
      "        [ 1.7774,  1.5116]])\n",
      "tensor([1, 1])\n",
      "--------------------------------------------------\n",
      "tensor([[-0.9382, -0.5430],\n",
      "        [-1.9629, -0.9923]])\n",
      "tensor([1, 0])\n",
      "--------------------------------------------------\n",
      "tensor([[-1.1402, -0.8388],\n",
      "        [-0.7206, -0.9606]])\n",
      "tensor([0, 0])\n",
      "--------------------------------------------------\n",
      "tensor([[ 1.8997,  0.8344],\n",
      "        [-2.8954,  1.9769]])\n",
      "tensor([1, 0])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "When working with raw datasets in PyTorch, the process begins by defining a custom `Dataset` class. This class serves as a blueprint for how the raw data will be accessed, transformed, and structured for use during training or evaluation. The flow typically starts in the `__init__` method of the `Dataset` class, where the raw data is loaded or prepared. For example, if the dataset consists of images stored in a directory, the `__init__` method might scan the directory, collect file paths, and store these paths in a list along with corresponding labels or metadata. At this point, the dataset exists in a structured format in memory or as references to external files, but no data is yet actively loaded into the system.\n",
    "\n",
    "When training or evaluating a model, the `DataLoader` class comes into play. The `DataLoader` wraps around the `Dataset` object and handles the complexities of batching, shuffling, and parallel data loading. For every batch, the `DataLoader` iterates over the dataset by repeatedly calling the `__getitem__` method of the `Dataset` class. The `__getitem__` method accesses individual data samples based on an index, which may involve loading a file from disk, applying transformations like resizing or normalization, and returning the processed data along with its label. This on-demand access is crucial for memory efficiency, especially for large datasets where preloading everything into memory would be infeasible.\n",
    "\n",
    "Batches are created by the `DataLoader` by grouping a fixed number of samples together, as specified by the `batch_size` parameter. The batching process involves calling `__getitem__` multiple times to fetch individual samples and then collating them into a single batch. This is where the collation function (default or custom) plays a role, ensuring that the samples in a batch are combined into a format suitable for model input, such as tensors of fixed size.\n",
    "\n",
    "If shuffling is enabled, the `DataLoader` randomizes the order in which indices are accessed. This happens at the beginning of each epoch, where the indices corresponding to the dataset samples are shuffled internally. This ensures that the model does not learn any spurious patterns based on the order of the data. The shuffled indices dictate how the `__getitem__` method is called during batch creation, ensuring that each batch contains samples selected randomly from the dataset.\n",
    "\n",
    "Additionally, the `DataLoader` can leverage multiple worker threads or processes to load data in parallel. This is especially useful for datasets stored on disk or requiring significant preprocessing, as it ensures that while one batch is being processed by the model, the next batch is being prepared simultaneously. This overlap between data preparation and model computation maximizes training efficiency. Once the batches are created, they are either passed to the model for training or returned to the user for analysis, completing the cycle."
   ],
   "id": "47d88afcc3d596b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5b50b30593ac50dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
